{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f09df18",
   "metadata": {},
   "source": [
    "topics I  covered\n",
    "->file_reading\n",
    "->schema\n",
    "->select\n",
    "->alias\n",
    "->where_filter\n",
    "->withcolumn and withcolumnrenamed\n",
    "->Typecasting\n",
    "->sort and order_by\n",
    "->limit\n",
    "->drop\n",
    "->drop_duplicates\n",
    "->union and union_by\n",
    "->string_funtion\n",
    "->date_func\n",
    "->handling_null\n",
    "->split and indexing\n",
    "->explode\n",
    "->array_contains\n",
    "->group_by\n",
    "->collect_list\n",
    "->pivot\n",
    "->when and otherwise\n",
    "->joins\n",
    "->window function\n",
    "->udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3285ce1e",
   "metadata": {},
   "source": [
    "##### File reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c58446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "\n",
    "df = spark.read.format('csv')\\\n",
    "               .option(\"inferSchema\",True)\\\n",
    "               .option(\"header\",True)\\\n",
    "               .load(\"C:/Git files/My git files/PySpark/files/test.csv\")\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216050f3",
   "metadata": {},
   "source": [
    "#### schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56d4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1,  \"date\": \"2025-01-03\", \"customer\": \"Arun\",     \"product\": \"Laptop\",      \"amount\": 56000},\n",
    "    {\"sale_id\": 2,  \"date\": \"2025-01-04\", \"customer\": \"Meena\",    \"product\": \"Mouse\",       \"amount\": 800},\n",
    "    {\"sale_id\": 3,  \"date\": \"2025-01-05\", \"customer\": \"John\",     \"product\": \"Keyboard\",    \"amount\": 1500},\n",
    "    {\"sale_id\": 4,  \"date\": \"2025-01-06\", \"customer\": \"Priya\",    \"product\": \"Monitor\",     \"amount\": 7200},\n",
    "    {\"sale_id\": 5,  \"date\": \"2025-01-07\", \"customer\": \"Sneha\",    \"product\": \"Laptop\",      \"amount\": 53000},\n",
    "    {\"sale_id\": 6,  \"date\": \"2025-01-08\", \"customer\": \"Kiran\",    \"product\": \"Tablet\",      \"amount\": 18000},\n",
    "    {\"sale_id\": 7,  \"date\": \"2025-01-09\", \"customer\": \"Rahul\",    \"product\": \"Earphones\",   \"amount\": 1200},\n",
    "    {\"sale_id\": 8,  \"date\": \"2025-01-10\", \"customer\": \"Vijay\",    \"product\": \"Speaker\",     \"amount\": 2500},\n",
    "    {\"sale_id\": 9,  \"date\": \"2025-01-11\", \"customer\": \"Anjali\",   \"product\": \"Webcam\",      \"amount\": 3000},\n",
    "    {\"sale_id\": 10, \"date\": \"2025-01-12\", \"customer\": \"Nitin\",    \"product\": \"Printer\",     \"amount\": 9000},\n",
    "    {\"sale_id\": 11, \"date\": \"2025-01-13\", \"customer\": \"Divya\",    \"product\": \"Laptop\",      \"amount\": 59000},\n",
    "    {\"sale_id\": 12, \"date\": \"2025-01-14\", \"customer\": \"Harish\",   \"product\": \"Mouse\",       \"amount\": 850},\n",
    "    {\"sale_id\": 13, \"date\": \"2025-01-15\", \"customer\": \"Suman\",    \"product\": \"Keyboard\",    \"amount\": 1600},\n",
    "    {\"sale_id\": 14, \"date\": \"2025-01-16\", \"customer\": \"Geeta\",    \"product\": \"Monitor\",     \"amount\": 7100},\n",
    "    {\"sale_id\": 15, \"date\": \"2025-01-17\", \"customer\": \"Mahesh\",   \"product\": \"Tablet\",      \"amount\": 17500},\n",
    "    {\"sale_id\": 16, \"date\": \"2025-01-18\", \"customer\": \"Asha\",     \"product\": \"Speaker\",     \"amount\": 2600},\n",
    "    {\"sale_id\": 17, \"date\": \"2025-01-19\", \"customer\": \"Lokesh\",   \"product\": \"Laptop\",      \"amount\": 60000},\n",
    "    {\"sale_id\": 18, \"date\": \"2025-01-20\", \"customer\": \"Tarun\",    \"product\": \"Webcam\",      \"amount\": 2800},\n",
    "    {\"sale_id\": 19, \"date\": \"2025-01-21\", \"customer\": \"Gauri\",    \"product\": \"Earphones\",   \"amount\": 1100},\n",
    "    {\"sale_id\": 20, \"date\": \"2025-01-22\", \"customer\": \"Sathish\",  \"product\": \"Printer\",     \"amount\": 9500}\n",
    "]\n",
    "\n",
    "df2 = spark.createDataFrame(data=sales_data)\n",
    "df2.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eca58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "my_struct_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "    \n",
    "])\n",
    "print(\"Struct_schema\")\n",
    "df2 = spark.read.format('csv')\\\n",
    "                .schema(my_struct_schema)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/sales.csv\")\n",
    "\n",
    "df2.printSchema()\n",
    "\n",
    "my_ddl_schema = \"\"\"\n",
    "sale_id INT,\n",
    "date STRING,\n",
    "customer STRING,\n",
    "product STRING,\n",
    "amount DOUBLE\n",
    "\"\"\"\n",
    "\n",
    "print(\"ddl_scehma\")\n",
    "df1 = spark.read.format('csv')\\\n",
    "                .schema(my_ddl_schema)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/sales.csv\")\n",
    "\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1b060",
   "metadata": {},
   "source": [
    "#### select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select('customer','amount').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9563903f",
   "metadata": {},
   "source": [
    "#### alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select(col(\"customer\").alias(\"customer_name\")).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121b2eb",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.filter(col(\"customer\").startswith('A')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590503d",
   "metadata": {},
   "source": [
    "#### withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5680f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumnRenamed(\"customer\",\"customer_name\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe9dab",
   "metadata": {},
   "source": [
    "#### Typecasting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn(\"date\",col(\"date\").cast(DateType()))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe403b1",
   "metadata": {},
   "source": [
    "#### sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86268bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort(col(\"customer_name\").desc()).show(4)\n",
    "df1.sort([\"sale_id\",\"customer_name\"],ascending=[0,1]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a259f",
   "metadata": {},
   "source": [
    "#### limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a24fa",
   "metadata": {},
   "source": [
    "#### Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(col(\"product\")).show(4)\n",
    "df1.drop(\"product\",\"amount\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f55c93",
   "metadata": {},
   "source": [
    "#### drop_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropDuplicates().show(4)\n",
    "df1.dropDuplicates(subset=[\"customer_name\",\"product\"]).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea184c31",
   "metadata": {},
   "source": [
    "#### union or unionByName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [\n",
    "        {\"sale_id\": 1, \"date\": \"2025-01-03\", \"customer\": \"Arun\", \"product\": \"Laptop\", \"amount\": 56000}, \n",
    "        {\"sale_id\": 2, \"date\": \"2025-01-04\", \"customer\": \"Meena\", \"product\": \"Mouse\", \"amount\": 800}\n",
    "        ]\n",
    "\n",
    "data2 =[\n",
    "    {\"sale_id\": 3, \"date\": \"2025-01-05\", \"customer\": \"John\", \"product\": \"Keyboard\", \"amount\": 1500}, \n",
    "    {\"sale_id\": 4, \"date\": \"2025-01-06\", \"customer\": \"Priya\", \"product\": \"Monitor\", \"amount\": 7200}, \n",
    "    {\"sale_id\": 5, \"date\": \"2025-01-07\", \"customer\": \"Sneha\", \"product\": \"Laptop\", \"amount\": 53000}\n",
    "]\n",
    "\n",
    "df_data1 = spark.createDataFrame(data=data1)\n",
    "df_data2 = spark.createDataFrame(data=data2)\n",
    "\n",
    "df= df_data1.union(df_data2)\n",
    "df.show()\n",
    "\n",
    "df= df_data1.unionByName(df_data2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976eed0",
   "metadata": {},
   "source": [
    "#### Srting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat\n",
    "df1 = df1.withColumn(\"full_name\",concat_ws(\" \",col(\"customer_name\"),lit(\"   AAA   \")))\n",
    "# df1.show(4)\n",
    "\n",
    "#trim\n",
    "df1 = df1.withColumn(\"trim_name\",trim(col(\"full_name\")))\n",
    "\n",
    "#ltrim\n",
    "df1 = df1.withColumn(\"ltrim_name\",ltrim(col(\"full_name\")))\n",
    "\n",
    "#rtrim\n",
    "df1 = df1.withColumn(\"rtrim_name\",rtrim(col(\"full_name\")))\n",
    "df1.show(4)\n",
    "df1 = df1.drop('full_name','trim_name','ltrim_name','rtrim_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea3399",
   "metadata": {},
   "source": [
    "#### Date fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0fdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_date\n",
    "df1 = df1.withColumn(\"current_date\",current_date())\n",
    "\n",
    "#date_add\n",
    "df1 = df1.withColumn(\"after_7_days\",date_add(\"current_date\",7))\n",
    "\n",
    "#date_sub\n",
    "df1 =df1.withColumn(\"before_7_days\",date_sub(\"current_date\",7))\n",
    "\n",
    "#date_diff\n",
    "df1 = df1.withColumn(\"date_diff\",date_diff(\"after_7_days\",\"current_date\"))\n",
    "\n",
    "#last_day\n",
    "df1 = df1.withColumn(\"last_day\",last_day(\"current_date\"))\n",
    "\n",
    "#date_format\n",
    "df1 = df1.withColumn(\"changed_date\",date_format(col('date'),'dd-MM-yyyy'))\n",
    "\n",
    "df1.select(\"date\",\"changed_date\",\"current_date\",\"after_7_days\",\"before_7_days\",\"date_diff\",\"last_day\",).show(4)\n",
    "df1 = df1.drop(\"changed_date\",\"current_date\",\"after_7_days\",\"before_7_days\",\"date_diff\",\"last_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98478d0b",
   "metadata": {},
   "source": [
    "#### handling null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66016d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dropna('any')\n",
    "df1.dropna('all')\n",
    "df1.dropna(subset=[\"customer_name\"])\n",
    "df1.fillna(\"not available\")\n",
    "df1.fillna(\"Not\",subset=[\"customer_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d722cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"file_read\").getOrCreate()\n",
    "data_f = spark.read.format('csv')\\\n",
    "                .option(\"inferSchema\",True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(\"C:/Git files/My git files/PySpark/files/friday_sale.csv\")\n",
    "data_f = data_f.withColumn(\"Full_name\",concat_ws(\" \",col(\"first_name\"),col(\"last_name\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511010f",
   "metadata": {},
   "source": [
    "#### split, indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350230c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = data_f.withColumn(\"split_name\",split(col(\"Full_name\"),\" \"))\n",
    "data_f = data_f.withColumn(\"f_name\",split(col(\"Full_name\"),\" \")[0])\\\n",
    "               .withColumn(\"l_name\",split(col(\"Full_name\"),\" \")[1])\n",
    "data_f.select(\"split_name\",\"f_name\",\"l_name\").show(4)\n",
    "data_f = data_f.drop(\"f_name\",\"l_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8cea2",
   "metadata": {},
   "source": [
    "#### Exploade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = data_f.withColumn(\"explode_data\",explode(col(\"split_name\")))\n",
    "data_f.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f1177",
   "metadata": {},
   "source": [
    "#### Array Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f = data_f.withColumn(\"array_contains\",array_contains(col(\"split_name\"),\"Kumar\"))\n",
    "data_f.show(4)\n",
    "data_f = data_f.drop(\"array_contains\",\"explode_data\",\"split_name\")\n",
    "data_f.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb902c4",
   "metadata": {},
   "source": [
    "#### Group By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f.groupBy(\"product\").agg(max(\"amount\").alias(\"maximun_amount\")\\\n",
    "                             ,min(\"amount\").alias(\"minmun_amount\")\\\n",
    "                             ,sum(\"amount\").alias(\"sum_amount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb9144",
   "metadata": {},
   "source": [
    "#### collect list,set,struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28990232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list,collect_set,struct\n",
    "\n",
    "spark = SparkSession.builder.appName(\"collect\").getOrCreate()\n",
    "\n",
    "student_hobbies = [\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Cricket\"},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Music\"},\n",
    "    {\"student_id\": 1, \"name\": \"Arun\",  \"hobby\": \"Chess\"},\n",
    "\n",
    "    {\"student_id\": 2, \"name\": \"Meena\", \"hobby\": \"Reading\"},\n",
    "    {\"student_id\": 2, \"name\": \"Meena\", \"hobby\": \"Dancing\"},\n",
    "\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Swimming\"},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Cricket\"},\n",
    "    {\"student_id\": 3, \"name\": \"Kiran\", \"hobby\": \"Gym\"},\n",
    "\n",
    "    {\"student_id\": 4, \"name\": \"Divya\", \"hobby\": \"Yoga\"},\n",
    "    {\"student_id\": 4, \"name\": \"Divya\", \"hobby\": \"Painting\"}\n",
    "]\n",
    "\n",
    "df_collect  = spark.createDataFrame(student_hobbies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db5baf",
   "metadata": {},
   "source": [
    "#### collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b753576",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collect.groupBy(\"student_id\",\"name\")\\\n",
    "          .agg(collect_list(\"hobby\").alias(\"hobbies\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54584455",
   "metadata": {},
   "source": [
    "#### collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collect.groupBy(\"student_id\",\"name\")\\\n",
    "          .agg(collect_set(\"hobby\").alias(\"hobbies\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bb0b9",
   "metadata": {},
   "source": [
    "#### struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b0d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collect.select(\"name\",struct(\"student_id\",\"hobby\").alias(\"details\"))\\\n",
    "          .groupBy(\"name\")\\\n",
    "          .agg(collect_list(\"details\").alias(\"details\"))\\\n",
    "          .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746394",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import first,when,col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pivot\").getOrCreate()\n",
    "\n",
    "employee_attendance = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-02\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",  \"date\": \"2025-01-03\", \"status\": \"Present\"},\n",
    "\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-02\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\", \"date\": \"2025-01-03\", \"status\": \"Present\"},\n",
    "\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-01\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-02\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 103, \"name\": \"Kiran\", \"date\": \"2025-01-03\", \"status\": \"Absent\"},\n",
    "\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-01\", \"status\": \"Present\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-02\", \"status\": \"Absent\"},\n",
    "    {\"emp_id\": 104, \"name\": \"Divya\", \"date\": \"2025-01-03\", \"status\": \"Present\"}\n",
    "]\n",
    "\n",
    "df_pivot  = spark.createDataFrame(employee_attendance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pivot.groupBy(\"emp_id\",\"name\")\\\n",
    "        .pivot(\"date\")\\\n",
    "        .agg(first(\"status\"))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c3c25",
   "metadata": {},
   "source": [
    "#### when otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d24ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"student_type\",when(\n",
    "    (col(\"2025-01-01\")==\"Present\")&\n",
    "    (col(\"2025-01-02\")==\"Present\")&\n",
    "    (col(\"2025-01-03\")==\"Present\"),\"good_student\").otherwise(\"bad_student\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d0b9a",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ed4cd",
   "metadata": {},
   "source": [
    "#### inner,left,right,anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f28e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"join\").getOrCreate()\n",
    "data1 = [\n",
    "  (1, \"Arun\",  20),\n",
    "  (2, \"Meena\", 22),\n",
    "  (3, \"Ravi\",  19),\n",
    "  (4, \"Kiran\", 21)\n",
    "]\n",
    "\n",
    "data2 =[\n",
    "  (1, \"Maths\",     88),\n",
    "  (1, \"Science\",   92),\n",
    "  (3, \"Maths\",     76),\n",
    "  (5, \"Science\",   81) \n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1,schema=['student_id',\"Name\",\"Age\"])\n",
    "df2 = spark.createDataFrame(data=data2,schema=['student_id',\"sub\",\"Marks\"])\n",
    "\n",
    "print(\"Inner_join\")\n",
    "df_inner = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='inner')\n",
    "df_inner.show()\n",
    "\n",
    "print(\"left_join\")\n",
    "df_left = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='left')\n",
    "df_left.show()\n",
    "\n",
    "print(\"right_join\")\n",
    "df_right = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='right')\n",
    "df_right.show()\n",
    "\n",
    "print(\"anti_join\")\n",
    "df_right = df1.join(df2,df1[\"student_id\"]==df2[\"student_id\"],how='anti')\n",
    "df_right.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf4e3d",
   "metadata": {},
   "source": [
    "#### Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a508010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, rank,dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "employee_perf = [\n",
    "    {\"emp_id\": 101, \"name\": \"Arun\",   \"dept\": \"IT\",        \"rating\": 4.3, \"projects\": 5},\n",
    "    {\"emp_id\": 102, \"name\": \"Meena\",  \"dept\": \"HR\",        \"rating\": 3.8, \"projects\": 3},\n",
    "    {\"emp_id\": 103, \"name\": \"Sachin\", \"dept\": \"Finance\",   \"rating\": 4.7, \"projects\": 6},\n",
    "    {\"emp_id\": 104, \"name\": \"Kiran\",  \"dept\": \"IT\",        \"rating\": 3.9, \"projects\": 2},\n",
    "    {\"emp_id\": 105, \"name\": \"Priya\",  \"dept\": \"Sales\",     \"rating\": 4.1, \"projects\": 4},\n",
    "    {\"emp_id\": 106, \"name\": \"Akash\",  \"dept\": \"Sales\",     \"rating\": 4.8, \"projects\": 7},\n",
    "    {\"emp_id\": 107, \"name\": \"Deepa\",  \"dept\": \"Finance\",   \"rating\": 3.6, \"projects\": 3},\n",
    "    {\"emp_id\": 108, \"name\": \"Varun\",  \"dept\": \"IT\",        \"rating\": 4.5, \"projects\": 5},\n",
    "    {\"emp_id\": 109, \"name\": \"Anita\",  \"dept\": \"HR\",        \"rating\": 3.7, \"projects\": 2},\n",
    "    {\"emp_id\": 110, \"name\": \"Rahul\",  \"dept\": \"Finance\",   \"rating\": 4.0, \"projects\": 4},\n",
    "    {\"emp_id\": 111, \"name\": \"Teena\", \"dept\": \"IT\", \"rating\": 4.3, \"projects\": 4}\n",
    "\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data=employee_perf)\n",
    "\n",
    "win = Window.partitionBy(\"dept\").orderBy(col(\"rating\").desc())\n",
    "\n",
    "df = df.withColumn(\"row_number\",row_number().over(win))\\\n",
    "       .withColumn(\"rank\",rank().over(win))\\\n",
    "       .withColumn(\"dense_rank\",dense_rank().over(win))\n",
    "\n",
    "df.filter((col(\"row_number\")==2) & (col(\"rank\")==2) & (col(\"dense_rank\")==2)).select(\"emp_id\",\"name\",\"dept\",\"rating\",\"projects\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"window\").getOrCreate()\n",
    "\n",
    "sales_data = [\n",
    "    {\"sale_id\": 1, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 5000},\n",
    "    {\"sale_id\": 2, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 7000},\n",
    "    {\"sale_id\": 3, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 6500},\n",
    "    {\"sale_id\": 4, \"region\": \"North\", \"salesperson\": \"Arun\",   \"amount\": 8000},\n",
    "    {\"sale_id\": 5, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 4000},\n",
    "    {\"sale_id\": 6, \"region\": \"South\", \"salesperson\": \"Meena\",  \"amount\": 9000},\n",
    "    {\"sale_id\": 7, \"region\": \"East\",  \"salesperson\": \"Sachin\", \"amount\": 7500},\n",
    "    {\"sale_id\": 8, \"region\": \"West\",  \"salesperson\": \"Priya\",  \"amount\": 6000},\n",
    "    {\"sale_id\": 9, \"region\": \"North\", \"salesperson\": \"Kiran\",  \"amount\": 3000},\n",
    "    {\"sale_id\": 10,\"region\": \"South\", \"salesperson\": \"Deepa\",  \"amount\": 5500}\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data=sales_data)\n",
    "\n",
    "win = Window.partitionBy(\"region\").orderBy(F.col(\"amount\").desc())\n",
    "win2 = Window.partitionBy(\"salesperson\").orderBy(F.col(\"amount\").desc())\n",
    "\n",
    "df1 = df1.withColumn(\"row_number\",F.row_number().over(win))\n",
    "\n",
    "# df1.filter(col(\"row_number\")==1).show()\n",
    "\n",
    "df1 = df1.groupBy(\"salesperson\") \\\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales\"))\n",
    "\n",
    "df1.show()\n",
    "win2 = Window.orderBy(F.col(\"total_sales\").desc())\n",
    "\n",
    "df1 = df1.withColumn(\"rank\", F.rank().over(win2))\n",
    "\n",
    "df1.filter((F.col(\"rank\")==1) | (F.col(\"rank\")==2)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f9be7",
   "metadata": {},
   "source": [
    "### UDF ->User defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f711852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"udf\").getOrCreate()\n",
    "\n",
    "products = [\n",
    "    {\"prod_id\": 1, \"product\": \"Laptop\",     \"category\": \"Electronics\", \"price\": 56000},\n",
    "    {\"prod_id\": 2, \"product\": \"Keyboard\",   \"category\": \"Electronics\", \"price\": 1500},\n",
    "    {\"prod_id\": 3, \"product\": \"Shoes\",      \"category\": \"Fashion\",     \"price\": 2500},\n",
    "    {\"prod_id\": 4, \"product\": \"T-Shirt\",    \"category\": \"Fashion\",     \"price\": 700},\n",
    "    {\"prod_id\": 5, \"product\": \"Mixer\",      \"category\": \"Home\",        \"price\": 3200},\n",
    "    {\"prod_id\": 6, \"product\": \"Sofa\",       \"category\": \"Home\",        \"price\": 28000}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(products)\n",
    "\n",
    "def price_range(x):\n",
    "    if x<2000:\n",
    "        return \"Low\"\n",
    "    elif x>=2000 and x<=10000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "    \n",
    "my_price_range = F.udf(price_range)\n",
    "\n",
    "df.withColumn(\"price_range\",my_price_range(F.col(\"price\"))).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
